---
title: " Ten common statistical mistakes to watch out for when writing or reviewing a manuscript"
subtitle: "by Tamar R Makin Andjean-Jacques Orban Dexivry"
format:
    revealjs:
        slide-number: true
        preview-links: true
        theme: default
---

#  About the Paper and it's Authors

##  About the Authors

Tamar R Makin (she/her)

- Neuroscientist studying brain plasticity (how the brain reorganizes itself)
- She is currently a professor at the University of Cambridge teaching cognitive neuroscience 

Andjean-Jacques Orban Dexivry

- Also a Neuroscientist studying movement control
- He is a professor in the Movement Control & Neuroplasticity research group of KU Leuven 

## The Inspiration for the Paper

- While the list was inspired by issues seen in neuroscience journals, the authors argue that this can 
be applied to any field of science
- There are 10 mistakes and for each, the authors provide an explanation of the problem, how reviewers 
can identify the problem, and how researchers can solve the problem
- There tends to be some overlap in the mistakes so it important to be aware of all these mistakes 
since one can impact the other


# Ten Most Common Statistical Mistakes 

## Absence of an adequate control condition/group

- Define the Problem: A control group might be present but there is some confounding variable present that impacts the variable being studied.
- " It is also important that the control and experimental groups are sampled at the same time and with randomised allocation, to minimise any biases."
- The control and experimental group should be identical (or at least close to identical) in every way except the variable of interest
- In order to make a claim about the effectiveness of some treatment/intervention/variable, there needs to be an adequate base line to 
compare it to
- How to Spot it: Either there is no control group that is being compared to, or there is a control group but key features are not accounted for
- How to fix it: "If the experimental design does not allow for separating the effect of time from the effect of the intervention, then conclusions regarding the
 impact of the intervention should be presented as tentative"

##  Interpreting comparisons between two effects without directly comparing them

- Define the Problem: When a difference occurs between the experimental and control group, the researchers will just assume that there is a significant difference without directly comparing the results
- " One can only conclude that the effect of an intervention is different from the effect of a control intervention through a direct statistical comparison between the two effects. Therefore, rather than running two separate tests, it is essential to use one statistical test to compare the two effects."
- Why is it a problem: See Erroneous analyses of interactions in neuroscience: a problem of significance (https://www.nature.com/articles/nn.2886)
- How to Spot it: There is a conclusion without directly comparing the two results
- How to fix it: Perform the approriate statistical analysis/test

##  Inflating the units of analysis

- Researchers sometimes mix up the number of subjects tested and the number of observations made overall (multiple can be made for each subject)
- Define the Problem: 
- Why is it a problem: this increases the degrees of freedom and therefore decreases the critical statistical threashold. This can result in results being interpreted as significant when that might not be the case. 
- How to Spot it: " If a study aims to understand group effects, then the unit of analysis should reflect the variance across subjects, not 
within subjects."
- How to fix it: Mixed-effects linear model or " calculate the correlation for each observation separately (e.g. pre, post) and
 interpret the R values based on the existing df. The researchers can also average the values across observations, or calculate the correlation
 for pre/post separately and then average the resulting R values (after applying normalisation of the R distribution, e.g. r-to-Z transformation),
 and interpret them accordingly."

## Spurious correlations

- Spurious correlations are when two variables have some sort of correlation but one variable does not directly cause the other
- Spurious correlations can arise from outliers or clusters
- Outliers should not just be discarded if they are valid observations. Consider using different statistical tools
- Define the Problem
- Why is it a problem
- How to Spot it: Take note of correlations that are reported without scatterplots or outliers that were thrown out without justification
- How to fix it: Consider using more robust correlation methods such as bootstrapping (resampling with replacement), data winsorizing (set outliers to a certain percentile), or skipped correlations (calculate correlations after removing outliers). If using parametric statistics, always consider assumption violations.

## Use of small samples

- Define the Problem: " When a sample size is small, one can only detect large effects, thereby leaving high uncertainty
 around the estimate of the true effect size and leading to an overestimation of the actual effect size"
- Why is it a problem: Small sample sizes are more susceptible to missing an effect that exists in the data (Type 2 error)
- How to Spot it: Be mindful of large claims based on small sample sizes
- How to fix it: If you have to use a small sample size, "efforts should be made to provide replications (both within and between
 cases) and to include sufficient controls"


##   Circular analysis

- Define the Problem: Circular analysis happens when you use the same data to define or select something and then test or measure that 
same thing. This creates a "circle" because you're using the data both to set up the analysis and to draw conclusions, which leads to 
biased or misleading results.
- Why is it a problem
- How to Spot it: There should be a justification to prove that the selected critieria and the area of interest are independent variables
- How to fix it: use bootstrapping, "use a different dataset (or different part of your dataset) for specifying the parameters for the 
analysis  and for testing your predictions", pre-define the analysis criteria

## Flexibility of analysis: p-hacking
- " But perhaps the best way to prevent p-hacking is to show some tolerance to borderline or non-significant results. In other words, if the
 experiment is well designed, executed, and analysed, reviewers should not ’punish’ the researchers for their data."
- Define the Problem: P-hacking is when there is manipulation done to the data anlysis process in order to obtain desired reults
- Why is it a problem: False positives
- How to Spot it: it is harded to spot. Sometimes it can be detected if there is a difference between the pre-planned analysis and the actual analysis.
- How to fix it: " Researchers should be transparent in the reporting of the results"


## Failing to correct for multiple comparisons

- Define the Problem: When testing a set of hypothesis, as the number of hypoteses tested increases, so do the chances of seeing a 
false positive (family-wise error rate). The more variables tested, the greater the chances of seeing a signifacant correlation with 
the dependent variable simply by chance
- Why is it a problem: increases the chance of type 1 error (false positive)
- How to Spot it: " Failing to correct for multiple comparisons can be detected by addressing the number of independent variables 
measured and the number of analyses performed. If only one of these variables correlated with the dependent variable, then the rest 
is likely to have been included to increase the chance of obtaining a significant result"
- How to fix it: " Exploratory testing can be absolutely appropriate, but should be acknowledged. Researchers
 should disclose all measured variables and properly implement the use of multiple comparison procedures. "

## Over-interpreting non-significant results
- " In simple words- non-significant effects could literally mean very different things- a true null result, an underpowered 
genuine effect, or an ambiguous effect "
- Define the Problem: 
- Why is it a problem: Just because you fail to reject the null hypothesis does not mean that the null hypothesis is true
- How to Spot it: Describing a non-significant p-value as indicating that an effect was not present
- How to fix it: "report effect sizes together with p-values in order to provide information about the magnitude of the effect"
- If applicable, use alternative statsitical methods such as Bayesian statistics or equivalence tests

## Correlation and causation

- Define the Problem: Determining that because two variables are correlated, one must cause the other
- Why is it a problem: Just because there is correlation, does not mean there is causation
- There may be other variables at play or it may just be a coincidence
- How to Spot it: " Researchers should only use causal language when a variable is precisely manipulated and even then, they should 
be cautious about the role of third variables or confounding factors."
- How to fix it: Look out for possible confounding varaiables

# Conclusion

## Summary

This presentation has covered:

- The ten most common statistical mistakes made in manuscripts
- How reviewers can spot each mistake
- How researchers can fix each mistake

## Further Reading



# THANK YOU!!!