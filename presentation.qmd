---
title: " Ten common statistical mistakes to watch out for when writing or reviewing a manuscript"
subtitle: "by Tamar R Makin Andjean-Jacques Orban Dexivry"
format:
    revealjs:
        slide-number: true
        preview-links: true
        theme: default
---

#  About the Paper and it's Authors

##  About the Authors

Tamar R Makin

- Neuroscientist studying brain plasticity (how the brain reorganizes itself)
- She is currently a professor at the University of Cambridge teaching cognitive neuroscience 

Andjean-Jacques Orban Dexivry

- Also a Neuroscientist studying movement control
- He is a professor in the Movement Control & Neuroplasticity research group of KU Leuven 

## The Inspiration for the Paper

- While the list was inspired by issues seen in neuroscience journals, the authors argue that this can 
be applied to any field of science
- There are 10 mistakes and for each, the authors provide an explanation of the problem, how reviewers 
can identify the problem, and how researchers can solve the problem
- There tends to be some overlap in the mistakes so it important to be aware of all these mistakes 
since one can impact the other


# Ten Most Common Statistical Mistakes 

## Absence of an adequate control condition/group

The Problem

- In cases where an effect is being studied over time, it is important to have a control group to 
compare the results to in order to have a baseline
- Sometimes, there either isn’t a control group or there is some bias introduced into the control 
group that makes the control group inadequate
- The control and experimental group should be as similar as possible, the only difference being 
the variable that is being manipulated

## Absence of an adequate control condition/group

Reviewers should look out for:

- Absence of control group
- Conclusions being drawn without being compared to the control group
- Not accounting for factors that may impact the control group

Solutions for researchers: 

- If you cannot separate the effects of time, then the results should be considered tentative


##  Interpreting comparisons between two effects without directly comparing them

The problem

- Researchers tend to determine the effect of some treatment/intervention by comparing the results 
of the experimental group to the control group to see if there was a significant difference
- The problem is when researchers conclude results based on the fact that the experimental group has 
significant results while the control group does not
- You cannot draw conclusions based on the fact that one group has statistically significant results 
and the other does not. They need to be directly compared to each other in one statistical test

##  Interpreting comparisons between two effects without directly comparing them

Reviewers should look out for:

- Conclusions are drawn without statistically comparing the two effects

For Researchers: 

- Compare groups directly
- Correlations between two groups can use Monte Carlo simulation
- When doing group comparisons, use ANOVA


##  Inflating the units of analysis

The Problem

- The experimental units used in analysis is important to be clearly defined because it 
determines the degrees of freedom used 
- A common mistake is when doing group analysis, the experimental unit is determined to 
be the number of observations made within each subject (this is incorrect), rather than 
the number of subjects (this is the correct unit)
- By not clearly defining and using the correct units of analysis, this can increase the degrees of freedom and therefore increase the statistical power. Thus, it is easier to observe falsely significant results

##  Inflating the units of analysis

Reviewers should look out for: 

- Experimental units that do not match the number of participants in the study

For Researchers:

- Consider using a mixed-effects linear model
- Calculate correlations for each observation separately
- Average values across observations
- Calculate correlations separately for pre and post, then average the resulting R values

## Spurious correlations

The Problem

- Correlation is an important tool that can help to understand the relationship between two variables
- In parametric correlation, Pearson’s correlation coefficient comes with a set of assumptions: normality, 
linearity, pairs of values, continuity, and no outliers
- Spurious correlation is when a relationship is detected that is not causal. This can often occur when there 
is a violation of one of the assumptions, the most common being the presence of outliers
- Outliers that did not occur by mistake should not simply be discarded. Instead, try another statistical approach

## Spurious correlations

Reviewers should look out for:

- Correlations being reported without a scatter plot
- Outliers that have been removed without justification

For Researchers:

- Try using alternative correlation metrics such as bootstrapping (resampling with replacement), data winsorizing 
(set outliers to a certain percentile), or skipped correlations (calculate correlations after removing outliers). 
- If using parametric statistics, always consider assumption violations.


## Use of small samples

The Problem

- With small sample sizes, it’s easier to detect only large effects, which creates a lot of uncertainty around the 
actual strength of the relationship between variables
- With small samples, false positives can appear large, leading to what’s called the "significance fallacy": the 
mistaken belief that a large effect in a small sample is definitely real. 
- In reality, a larger correlation in small samples isn’t due to a stronger relationship; it’s just an overestimate 
due to the small sample size
- Small sample sizes can also make it difficult to test for normality

## Use of small samples

Reviewers should look out for:

- Strong claims made using a small sample size
- It is the reviewers discretion on whether or no the sample size is too small

For Researchers:

- Provide evidence that the sample size is reasonable through performing a statistical power analysis or through replicating the study
- If the sample size is limited, use Crawford t-test (for case studies), perform replications of the study, try to 
add in controls to isolate the effect of the primary factor being studied


##   Circular analysis

- Define the Problem: Circular analysis happens when you use the same data to define or select something and then test or measure that 
same thing. This creates a "circle" because you're using the data both to set up the analysis and to draw conclusions, which leads to 
biased or misleading results.
- Why is it a problem
- How to Spot it: There should be a justification to prove that the selected critieria and the area of interest are independent variables
- How to fix it: use bootstrapping, "use a different dataset (or different part of your dataset) for specifying the parameters for the 
analysis  and for testing your predictions", pre-define the analysis criteria

## Flexibility of analysis: p-hacking
- " But perhaps the best way to prevent p-hacking is to show some tolerance to borderline or non-significant results. In other words, if the
 experiment is well designed, executed, and analysed, reviewers should not ’punish’ the researchers for their data."
- Define the Problem: P-hacking is when there is manipulation done to the data anlysis process in order to obtain desired reults
- Why is it a problem: False positives
- How to Spot it: it is harded to spot. Sometimes it can be detected if there is a difference between the pre-planned analysis and the actual analysis.
- How to fix it: " Researchers should be transparent in the reporting of the results"


## Failing to correct for multiple comparisons

- Define the Problem: When testing a set of hypothesis, as the number of hypoteses tested increases, so do the chances of seeing a 
false positive (family-wise error rate). The more variables tested, the greater the chances of seeing a signifacant correlation with 
the dependent variable simply by chance
- Why is it a problem: increases the chance of type 1 error (false positive)
- How to Spot it: " Failing to correct for multiple comparisons can be detected by addressing the number of independent variables 
measured and the number of analyses performed. If only one of these variables correlated with the dependent variable, then the rest 
is likely to have been included to increase the chance of obtaining a significant result"
- How to fix it: " Exploratory testing can be absolutely appropriate, but should be acknowledged. Researchers
 should disclose all measured variables and properly implement the use of multiple comparison procedures. "

## Over-interpreting non-significant results
- " In simple words- non-significant effects could literally mean very different things- a true null result, an underpowered 
genuine effect, or an ambiguous effect "
- Define the Problem: 
- Why is it a problem: Just because you fail to reject the null hypothesis does not mean that the null hypothesis is true
- How to Spot it: Describing a non-significant p-value as indicating that an effect was not present
- How to fix it: "report effect sizes together with p-values in order to provide information about the magnitude of the effect"
- If applicable, use alternative statsitical methods such as Bayesian statistics or equivalence tests

## Correlation and causation

- Define the Problem: Determining that because two variables are correlated, one must cause the other
- Why is it a problem: Just because there is correlation, does not mean there is causation
- There may be other variables at play or it may just be a coincidence
- How to Spot it: " Researchers should only use causal language when a variable is precisely manipulated and even then, they should 
be cautious about the role of third variables or confounding factors."
- How to fix it: Look out for possible confounding varaiables

# Conclusion

## Summary

This presentation has covered:

- The ten most common statistical mistakes made in manuscripts
- How reviewers can spot each mistake
- How researchers can fix each mistake

## Further Reading



# THANK YOU!!!