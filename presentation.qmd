---
title: " Ten common statistical mistakes to watch out for when writing or reviewing a manuscript"
subtitle: "by Tamar R Makin Andjean-Jacques Orban Dexivry"
format:
    revealjs:
        slide-number: true
        preview-links: true
        theme: default
---

#  About the Paper and it's Authors

##  About the Authors

Tamar R Makin

- Neuroscientist studying brain plasticity (how the brain reorganizes itself)
- She is currently a professor at the University of Cambridge teaching cognitive neuroscience 

Andjean-Jacques Orban Dexivry

- Also a Neuroscientist studying movement control
- He is a professor in the Movement Control & Neuroplasticity research group of KU Leuven 

## The Inspiration for the Paper

- While the list was inspired by issues seen in neuroscience journals, the authors argue that this can 
be applied to any field of science
- There are 10 mistakes and for each, the authors provide an explanation of the problem, how reviewers 
can identify the problem, and how researchers can solve the problem
- There tends to be some overlap in the mistakes so it important to be aware of all these mistakes 
since one can impact the other


# Ten Most Common Statistical Mistakes 

## Absence of an adequate control condition/group

The Problem

- In cases where an effect is being studied over time, it is important to have a control group to 
compare the results to in order to have a baseline
- Sometimes, there either isn’t a control group or there is some bias introduced into the control 
group that makes the control group inadequate
- The control and experimental group should be as similar as possible, the only difference being 
the variable that is being manipulated

## Absence of an adequate control condition/group

Reviewers should look out for:

- Absence of control group
- Conclusions being drawn without being compared to the control group
- Not accounting for factors that may impact the control group

Solutions for researchers: 

- If you cannot separate the effects of time, then the results should be considered tentative


##  Interpreting comparisons between two effects without directly comparing them

The problem

- Researchers tend to determine the effect of some treatment/intervention by comparing the results 
of the experimental group to the control group to see if there was a significant difference
- The problem is when researchers conclude results based on the fact that the experimental group has 
significant results while the control group does not
- You cannot draw conclusions based on the fact that one group has statistically significant results 
and the other does not. They need to be directly compared to each other in one statistical test

##  Interpreting comparisons between two effects without directly comparing them

Reviewers should look out for:

- Conclusions are drawn without statistically comparing the two effects

For Researchers: 

- Compare groups directly
- Correlations between two groups can use Monte Carlo simulation
- When doing group comparisons, use ANOVA


##  Inflating the units of analysis

The Problem

- The experimental units used in analysis is important to be clearly defined because it 
determines the degrees of freedom used 
- A common mistake is when doing group analysis, the experimental unit is determined to 
be the number of observations made within each subject (this is incorrect), rather than 
the number of subjects (this is the correct unit)
- By not clearly defining and using the correct units of analysis, this can increase the degrees of freedom and therefore increase the statistical power. Thus, it is easier to observe falsely significant results

##  Inflating the units of analysis

Reviewers should look out for: 

- Experimental units that do not match the number of participants in the study

For Researchers:

- Consider using a mixed-effects linear model
- Calculate correlations for each observation separately
- Average values across observations
- Calculate correlations separately for pre and post, then average the resulting R values

## Spurious correlations

The Problem

- Correlation is an important tool that can help to understand the relationship between two variables
- In parametric correlation, Pearson’s correlation coefficient comes with a set of assumptions: normality, 
linearity, pairs of values, continuity, and no outliers
- Spurious correlation is when a relationship is detected that is not causal. This can often occur when there 
is a violation of one of the assumptions, the most common being the presence of outliers
- Outliers that did not occur by mistake should not simply be discarded. Instead, try another statistical approach

## Spurious correlations

Reviewers should look out for:

- Correlations being reported without a scatter plot
- Outliers that have been removed without justification

For Researchers:

- Try using alternative correlation metrics such as bootstrapping (resampling with replacement), data winsorizing 
(set outliers to a certain percentile), or skipped correlations (calculate correlations after removing outliers). 
- If using parametric statistics, always consider assumption violations.


## Use of small samples

The Problem

- With small sample sizes, it’s easier to detect only large effects, which creates a lot of uncertainty around the 
actual strength of the relationship between variables
- With small samples, false positives can appear large, leading to what’s called the "significance fallacy": the 
mistaken belief that a large effect in a small sample is definitely real. 
- In reality, a larger correlation in small samples isn’t due to a stronger relationship; it’s just an overestimate 
due to the small sample size
- Small sample sizes can also make it difficult to test for normality

## Use of small samples

Reviewers should look out for:

- Strong claims made using a small sample size
- It is the reviewers discretion on whether or no the sample size is too small

For Researchers:

- Provide evidence that the sample size is reasonable through performing a statistical power analysis or through replicating the study
- If the sample size is limited, use Crawford t-test (for case studies) or try to add in controls to isolate the effect of the 
primary factor being studied


## Circular analysis

- Circular analysis happens when you use the same data to define or select something and then test or measure that same thing. 
This creates a "circle" because you're using the data both to set up the analysis and to draw conclusions, which leads to biased 
or misleading results.
- A hypothesis is tested on the same data set that led to the generation of the hypothesis in the first place

## Circular analysis

Reviewers should look out for:

- Data that was selected because it showed the effect of interest
- If an effect size (such as a correlation or a difference between groups) is much larger than what is theoretically or practically plausible (for example, finding a very high correlation between two variables that are known to have only a weak or moderate relationship in previous studies might be suspicious)

For researchers: 

- Before looking at the data, decide the analysis criteria to avoid introducing bias
- Use a different dataset to test your predictions


## Flexibility of analysis: p-hacking

The Problem

- P-hacking is when researchers manipulate the procedure in order to get a significant result
- This can come from removing data points, trying out multiple tests to get a significant result, removing variables, etc.
- The authors argue that if the experiment is well designed, there should be some tolerance towards borderline significant results

## Flexibility of analysis: p-hacking

Reviewers should look out for:

- If a large bulk of variables were taken but only a few statistically significant ones were reported
- If the analysis plan is predetermined before testing and if that plan was followed

For researchers:

- Avoid changing hypothesis before testing
- Be transparent
- Use pre-registration of the experiment design and analysis


## Failing to correct for multiple comparisons

- Define the Problem: When testing a set of hypothesis, as the number of hypoteses tested increases, so do the chances of seeing a 
false positive (family-wise error rate). The more variables tested, the greater the chances of seeing a signifacant correlation with 
the dependent variable simply by chance
- Why is it a problem: increases the chance of type 1 error (false positive)
- How to Spot it: " Failing to correct for multiple comparisons can be detected by addressing the number of independent variables 
measured and the number of analyses performed. If only one of these variables correlated with the dependent variable, then the rest 
is likely to have been included to increase the chance of obtaining a significant result"
- How to fix it: " Exploratory testing can be absolutely appropriate, but should be acknowledged. Researchers
 should disclose all measured variables and properly implement the use of multiple comparison procedures. "

## Over-interpreting non-significant results
- " In simple words- non-significant effects could literally mean very different things- a true null result, an underpowered 
genuine effect, or an ambiguous effect "
- Define the Problem: 
- Why is it a problem: Just because you fail to reject the null hypothesis does not mean that the null hypothesis is true
- How to Spot it: Describing a non-significant p-value as indicating that an effect was not present
- How to fix it: "report effect sizes together with p-values in order to provide information about the magnitude of the effect"
- If applicable, use alternative statsitical methods such as Bayesian statistics or equivalence tests

## Correlation and causation

- Define the Problem: Determining that because two variables are correlated, one must cause the other
- Why is it a problem: Just because there is correlation, does not mean there is causation
- There may be other variables at play or it may just be a coincidence
- How to Spot it: " Researchers should only use causal language when a variable is precisely manipulated and even then, they should 
be cautious about the role of third variables or confounding factors."
- How to fix it: Look out for possible confounding varaiables

# Conclusion

## Summary

This presentation has covered:

- The ten most common statistical mistakes made in manuscripts
- How reviewers can spot each mistake
- How researchers can fix each mistake

## Further Reading



# THANK YOU!!!